1.信息论基础：
           联合熵：衡量一组变量的不确定性。
           条件熵：定义为X给定条件下，Y的条件概率分布的熵对X的数学期望。
           信息增益 ：表示得知特征X的信息而使得类Y的信息的不确定性减少的程度。
           基尼不纯度：是指将来自集合中的某种结果随机应用在集合中，
某一数据项的预期误差率。在进行决策树编程的时候，可以作为衡量系统混乱程度的标准。


2.决策树算法(ID3、C4.5、CART分类树)原理及应用场景：
           ID3算法：在ID3算法中，选择信息增益最大的属性作为当前的特征对数据集分类，
           通过不断的选择特征对数据集不断划分。但只能处理离散型属性。
           C4.5：
           算法流程与ID3相类似，只不过将信息增益改为信息增益比，以解决偏向取值较多的属性的问题，
           另外它可以处理连续型属性。
           CART分类树：CART是一棵二叉树，采用二元切分法，每次把数据切成两份，
           分别进入左子树、右子树。而且每个非叶子节点都有两个孩子，
           所以CART的叶子节点比非叶子多1。相比ID3和C4.5，CART应用要多一些，
           既可以用于分类也可以用于回归。
 由两部分组成：
（1）决策树生成：基于训练数据集生成决策树，生成的决策树要尽量大；
（2）决策树剪枝：用验证数据集对已生成的数进行剪枝并选择最优子树，这时用损失函数最小作为剪枝的标准。

3.回归树原理：
            使用最大均方差划分节点；每个节点样本的均值作为测试样本的回归预测值。
4.决策树防止过拟合手段：
            剪枝：对决策树进行剪枝来简化学到的决策树。从已生成的树上剪掉一些叶结点或叶结点以上的子树，
            生成：并将其父结点或根结点作为新的叶结点，从而简化生成的决策树。
5.模型评估：
            1、随机二次抽样：多次重复保持方法来改进对分类器性能的估计
            2、交叉验证：每个记录用于训练的次数相同，并且恰好检验一次。
            3、自助法：训练记录采用有放回抽样，即已经选作训练的记录将放回原来的记录集中，使得它等机率地被重新抽取。
            如果原始数据有N个记录，可以证明，平均来说，大小为N的自助样本大约包含原始数据中63.2%的记录。
 6.sklearn参数：
            criterion
            默认基尼系数，写’entropy’使用信息增益。
            一般，数据维度大、噪音大的时候，用基尼系数；感觉模型拟合程度不够的时候，用信息熵。

            random_state
            整数，随机数种子，确定之后不管运行多少次这个树不会变。

            splitter
            默认"best"，可以改成"random"，树就会更大。
            特征切分点选择标准，决策树是递归地选择最优切分点，spliter是用来指明在哪个集合上来递归，有“best”和“random”两种参数可以选择，best表示在所有特征上递归，
            适用于数据集较小的时候，random表示随机选择一部分特征进行递归，适用于数据集较大的时候。

           
           以下参数用于剪枝，防止过拟合的关键：
           max_depth
最大深度，最常用的参数，适用于高纬度样本量较少。一般可以从3开始试。
           min_samples_split
一个节点想再往下分，需要包含的最少样本数。
整数，就是最小数目；浮点数，分完每个节点样本数的最小比例。

属性
feature_importances_
每个特征的重要性。
方法
xx.apply()
返回每条数据最终落在哪个叶子节点。
xx.fit()
返回建好的模型对象。
xx.predict()
返回每条数据预测的标签。
xx.score()
返回accuracy值
            
