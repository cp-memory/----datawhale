1.
监督和无监督学习的区别：
监督学习：前者训练集每条数据有对立正确的答案.
无监督学习：算法自动地聚集那些个体到各个类，自动地找到数据中的结构。

泛化能力：学得模型适用于新样本的能力.

过拟合和欠拟合以及解决方法：
判断依据：根据训练集和测试集两者的误差，去判断是过拟合还是欠拟合。
解决方法：
        过拟合：增加正则化参数程度，收集更多数据，
        欠拟合：获得更多特征，增加多项式特征，减少正则化参数程度。
        
交叉验证：需要用交叉验证集来帮助选择模型。
         模型选择的方法：使用训练集训练出10个模型
         用 10 个模型分别对交叉验证集计算得出交叉验证误差（代价函数的值）
         选取代价函数值最小的模型
         用上个步骤选出的模型对测试集计算得出推广误差（代价函数的值）

2.线性回归原理：对一系列数据进行拟合，寻找一般的规律。

3.线性回归损失函数、代价函数、目标函数：

        损失函数：定义在单个样本上的，算单个样本的误差。
        代价函数：定义在整个训练集上的，是所有样本误差的平均。
        目标函数定义：拟合出最好的函数；等于经验风险+结构风险（也就是Cost Function + 正则化项）。
                     关于目标函数和代价函数还有一种通俗的区别：目标函数是最大化或者最小化，而代价函数是最小化

4.优化方法(梯度下降法、牛顿法、拟牛顿法等)：
         梯度下降法：进行多次迭代，参数对loss function进行微分，参数减去每次迭代的微分值并进行更新，最后到达代价函数最小值。
         牛顿法：无约束条件的最优化问题，近似求解的方法；使用函数f (x)的泰勒级展开前面几项来寻找方程f (x) = 0的根。
         拟牛顿法：对牛顿法进一部的改善，简化了运算的复杂度。计算每一次迭代时参数对loss function的梯度，构建关于一个目标函数的模型。
5.线性回归的评估指标：
均方误差：预测与实际值相减的结果进行平方。
均方根误差：在均方误差结果下进一步开根号。
平均绝对误差：预测值与实际值相减并进行绝对值的结果。

6.sklearn参数详解：
      from sklearn.linear_model import LinearRegression
LinearRegression(fit_intercept=True,normalize=False,copy_X=True,n_jobs=1)
'''
        参数含义：
        1.fit_intercept:布尔型，默认为true
        是否对训练数据进行中心化。如果该变量为false，则表明输入的数据已经进行了中心化，
        在下面的过程里不进行中心化处理；否则，对输入的训练数据进行中心化处理。
        2.normalize:布尔型。为False的话，训练样本会进行归一化处理。
        3.copy_X：布尔型。为True的话，复制一份训练数据。
        4.n_jobs:整型。任务并行时指定的CPU数量。如果取值为-1则使用所有可用的CPU。
        5.coef_:数组型变量，权重向量。
        6.intercept_:数组型变量，截距。
 






